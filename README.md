# Applatix Service YAML Spec
This document describes the YAML format that serves as the DSL for the Applatix devops workflows and policies. 

## Overview
With Applatix, workflows and policies are specified as YAML documents and checked into source control (e.g. git). They behave like code and are treated as code. The workflows can call each other like functions. They can change and drift apart in different branches, and be made consistent again when merged back to the master branch. We also provide a syntax and consistency checker to validate the YAML code. 

## Examples
Let's start with a few simple examples to get a feel for the capabilities.

### Service that Runs a Container
The following is a simple service that checks out source code. It uses the `axscm:v1` docker image in the private `get.applatix.io` repository, and runs the `axscm` command to checkout the code to the `/src` directory inside the container. The `repo` and `commit` parameters are provided by the GUI session, and the code in `/src` is exposed as an artifact called `code`.

```yaml
---
name: axscm-checkout
type: service_template
subtype: container
description: Checks out a source repository to /src
container:
  resources:
    mem_mib: 256
    cpu_cores: 0.1
  image: get.applatix.io/applatix/axscm:v1
  command: "axscm clone %%repo%% /src --commit %%commit%%"
inputs:
  parameters:
    commit:
      default: "%%session.commit%%"
    repo:
      default: "%%session.repo%%"
outputs:
  artifacts:
    code:
      path: "/src"
labels:
  release: 1.0.1
  milestone: m5
```

The next example describes a service that does a build. This service needs an input artifact from another `step` in the parent workflow. This artifact will be unpacked to the path `/src` inside the container before it begins execution.

```yaml
---
name: golang-example-base
type: service_template
subtype: container
description: golang example base container
container:
  resources:
    mem_mib: 256
    cpu_cores: 0.1
  image: "get.applatix.io/applatix/golang:1.6"
  command: "%%CMD%%"
inputs:
  artifacts:
  - from: "%%code_artifact%%"
    path: "/src"
  parameters:
    CMD:
    code_artifact:
```

### A Simple Workflow

The following is a workflow that does a checkout followed by a build of the checked out source code.  `CMD` is the build command that is run to perform the build.  The build step's `code` partifact passed in as the `%%code_artifact%%` so that the build step can access the artifacts generated by the checkout step.

```yaml
---
name: golang check and build
type: service_template
subtype: workflow
description: golang example workflow that does checkout and build
inputs:
  parameters:
    commit:
      default: "%%session.commit%%"
    repo:
      default: "%%session.repo%%"
steps:
- checkout:
    template: axscm-checkout
- build:
    template: golang-example-base
    parameters:
      CMD: "sh -c 'cd /src && go build main.go'"
      code_artifact: "%%steps.checkout.code%%"
labels:
  release: 1.0.2
  milestone: m6
```

### A Simple Policy

The following is a policy that triggers `golang check and build` on several events.

```yaml
---
type: policy
name: Sample Policy
description: Sample Policy to trigger golang example
template: golang check and build
parameters:
# parameters with "%%session.*%%" as default value can be full filled automatically, others
# need to be specified in this section
notifications:
# multiple notification can be sepecified
  -
    when:
      # options: on_start, on_success, on_failure, on_change 
      - on_start
      - on_success
      - on_failure
    whom:
      # options: committer, author, email address, user label
      - committer
      - author
      - abc@company.com
  -
    when:
      - on_change
    whom:
      - abc@company.slack.com
when:
# multiple triggers can be specified
  -
    # options: on_push, on_pull_request, on_pull_request_merge, on_cron
    event: on_push
    target_branches:
      # target_branches are in regular expression format, eg. ".*" matches all branches
      - "master"
      - "dev.*"
  -
    event: on_pull_request
    target_branches:
      - ".*"
  -
    event: on_pull_request_merge
    target_branches:
      - ".*"
  -
    event: on_cron
    target_branches:
      - ".*"
    # cron expression
    # 0 1 * * *
    #	| | | | |
    #	| | | | |
    #	| | | | +---- Run every day of the week
    #	| | | +------ Run every month of the year
    #	| | +-------- Run every day of the month
    #	| +---------- Run at 1 Hour (1AM)
    #	+------------ Run at 0 Minute
    schedule: "0 */2 * * *"
    timezone: "US/Pacific"
labels:
  release: 1.0.2
  milestone: m6
```

## DSL Spec

### Service Template
A service template has the following sections.

#### Common Name and Type
* name: the unique name of the object.
* type: service_template.
* subtype: container, workflow
* description: optional, describes the object for the benefit of the users.

#### inputs
There are two categories of inputs. The parameters and the artifacts.

* parameters: it is a map of parameter name to parameter object mapping. Each parameter can be used in the YAML body in the form of %%parameter name%% to be replaced with the parameter value at run time.
	* Parameter object has the following fields.
		* default: the default value of the parameter. Optional. If a default value is specified and the user didn't provide a value when launching a service instance, the default value will be used.
		* description: optional, a description for the benefit of the users of the service.
* artifacts: it is an array of artifact objects. Each input artifact is placed at a specified path.
	* Artifact object has the following fields.
		* path: the absolute path inside the container where this artifact will be placed.
		* from: where the artifact comes from. The common pattern is in the form of %%xxx_artifactr%% where %%xxx_artifact%% is passed in as a parameter from the parent workflow.

#### outputs
We currently support only artifact outputs.

* artifacts: it's a map of unique output artifact name to artifact object mapping.
* Artifact objects has the following fields.
	* path: the absolute inside a container that we will export as artifact.
	* excludes: an array of file name patterns that we will exclude from the artifact collection. The pattern is in the from of tar pattern.

#### container
This section describes a docker container.

* image: the docker container image. Same as what you would provide to docker pull.
* command: a string that specifies the command to run in the container.
* docker_options: the docker options. We only support "-e", "-p" options, though the use of "-p" is strongly discouraged, because it limits the migratability of the docker container.
* resources: the system resource the container is allowed to use.
	* mem_mib: amount of memory in MiB the service is expected to use. We will only run this service on a host with that much free memory. We allow the service's memory to grow to 1.5x of specified amount, then it will be killed.
	* cpu_cores: the percentage of a cpu core we allow the service to use. 0.1 means 10% of a single core. We cap the service to use up to 4x of the specified amount. The service will not be killed for exceeding cpu limit.

#### steps
This section describes the steps in a workflow. It's mutually exclusive to the container section. A workflow only uses other containers and workflows, and never defines a container itself.

A workflow can container any number of steps in an array. The steps are executed in sequence, and if one step fails, we fail the workflow and abort the following steps.

Each step can also contain multiple tasks, which will be run in parallel. If any one of them fails, the others in the same step will still execute to completion, but the overall step will fail.

The steps section contains

* An array of step objects. Each step object contains
	* A map of task name to task object mapping. The task name must be unique within the whole workflow, not just within the step.
	* The task object contains the following fields.
		* template: the name of the service template this task runs.
		* parameters: it's a map of parameter name to value mapping. The parameter names are as defined in the service template.

#### exception handling
In many cases, we will want to specially handle errors and exceptions that occur during workflows.
Two special flags are provided for this purpose.
* ignore_error: Ignore any errors/failures for the specified step. The step always succeeds.
* always_run: Always run this step before exiting the current sequence of steps.i

Below, the cleanup step will always be run even if the job fails or is canceled during the setup or test phase. Furthermore, any errors during the cleanup step will be ignored and cannot fail the workflow.

```yaml
---
steps:
- setup:
    template: mysetup
- test:
    template: mytest
- cleanup:
    template: mycleanup
    flags:
        always_run: true
        ignore_error: true
```

#### fixtures
This section defines the fixtures that this service can launch or reserve. There are two types of fixtures: static and dynamic.

* static - static fixtures are externally managed resources which are requested and reserved during a portion of a workflow. Static fixtures are inputed into the system through the UI, along with user defined attributes. A typical use case for static fixtures, would be to have a pool of hosts or VMs in which tests require exclusive access to during execution. Upon completion of the workflow, the reserved fixtures are released back into the pool to be used by other workflows.
* dynamic - dynamic fixtures are containers that run alongside all steps of a workflow. Dynamic fixtures are different from a regular task in the sense that its life spans the whole service that runs it. Typically a dynamic fixture wouldn't exit on its own, and will be shut down by the Applatix framework at the end of the service that uses the fixture.

The fixture section is structure similar to the steps section. 

* An array of fixture steps we execute. They are run in sequence. Each step contains:
	* A map of fixture name to fixture object. The fixture name must be unique within the workflow.
	* A dynamic fixture object contains the following:
		* template: (required) the name of the service template that this fixture runs.
		* parameters: the map of name, value pairs to satisfy the above template's parameter requirements.
		* probe: an optional http REST endpoint that the fixture can expose to let the workflow engine know that it's ready. Once specified, the workflow engine will poll http://<container_ip>:<port>/<path>, and will only proceed when it returns HTTP status 200.
			* port: the port the endpoint runs on.
			* path: the URL path the endpoint responds to.
	* A static fixture object contains a map of requirements to be requested during workflow execution:
	   * category: the category name to request
	   * name: the name of a specific fixture
	   * attributes: a map of one or more fixture attributes to request (e.g. os_vendor: "Ubuntu")

Below is an example of a test using a mongodb dynamic fixture. For simplicity we didn't include the container definition of the `mongodb-fixture` or `mogodb-loadgen` service templates.

```yaml
name: mongodb-test-workflow
type: service_template
subtype: workflow
fixtures:
- mongodb_fixture:
    template: mongodb-fixture
steps:
- mytest:
    template: mogodb-loadgen
    parameters:
      db_address: "%%fixtures.mongodb_fixture.ip%%
```
A working example of a repository using dynamic fixtures can be found at: https://github.com/Applatix/example-dynamic-fixtures

Below is an example of a test requesting a static fixture, specifically requesting a Linux host running Ubuntu 16.04. The `Linux` category, and attributes `os_vendor`, `os_version`, `hostname` are all user defined attributes that have been inputed into the system via the GUI.

```yaml
name: linux-test-workflow
type: service_template
subtype: workflow
fixtures:
- linux_fixture:
    category: Linux
    attributes:
       os_vendor: Ubuntu
       os_version: 16.04
steps:
- mytest:
    template: linux-test
    parameters:
      hostname: "%%fixtures.linux_fixture.hostname%%

```

#### static fixtures ####
A static fixture is a xxx

#### Parameter Special Values
The parameters can have the following special values:

* %%session.commit%% - the commit that the user selected on the UI
* %%session.repo%% - the repo for the above commit
* %%steps.<name>%% - the step identified by name
* %%steps.<name>.ip%% - the IP address of the service associated with the step
* %%steps.<name>.<artifact_name>%% - the artifact called "artifact_name" generated by the step
* %%fixtures.<name>%% - the fixture identifed by name
* %%fixtures.<name>.ip%% - the fixture's IP address

### Policy

A policy has the following sections. 

Follow the steps to enable a policy in a branch:

* Make sure the policy `when` `target_branches` can match the branch name
* Enable the policy from the Policies Page 

#### Common Name and Type
* name: the unique name of the object.
* type: policy.
* description: optional, describes the object for the benefit of the users.
* template: the template name

```yaml
---
type: policy
name: Sample Policy
description: Sample Policy to trigger golang example
template: golang check and build
```

#### parameters
* Parameters with "%%session.commit%%" or "%%session.repo%%" as default value can be full filled automatically, they don't need to be specified here
* Others need to be specified in this section to full file the template parameters section

#### notifications
```yaml
notifications:
# multiple notification can be sepecified
  -
    when:
      # options: on_start, on_success, on_failure, on_change 
      - on_start
      - on_success
      - on_failure
    whom:
      # options: committer, author, email address, user label
      - committer
      - author
      - abc@company.com
  -
    when:
      - on_change
    whom:
      - abc@company.slack.com
```

* Notification is made of two parts
	* `when` specifies the list of job event to notify, options: `on_start`, `on_success`, `on_failure`, `on_change`
	* `whom` specifies the list of destination to notify, options: `committer`, `author`, `email address`, `user label`
* Multiple notification can be specified

### when
```yaml
when:
# multiple triggers can be specified
  -
    # options: on_push, on_pull_request, on_pull_request_merge, on_cron
    event: on_push
    target_branches:
      # target_branches are in regular expression format, eg. ".*" matches all branches
      - "master"
      - "dev.*"
  -
    event: on_pull_request
    target_branches:
      - ".*"
  -
    event: on_pull_request_merge
    target_branches:
      - ".*"
  -
    event: on_cron
    target_branches:
      - ".*"
    # cron expression
    # 0 1 * * *
    #	| | | | |
    #	| | | | |
    #	| | | | +---- Run every day of the week
    #	| | | +------ Run every month of the year
    #	| | +-------- Run every day of the month
    #	| +---------- Run at 1 Hour (1AM)
    #	+------------ Run at 0 Minute
    schedule: "0 */2 * * *"
    timezone: "US/Pacific"
```

* when has different event types, for all event, `target_branches` are required and in regular expression format
	* `on_push`: trigger on the SCM push event
	* `on_pull_request`: trigger on the pull request creation/update
	* `on_pull_request_merge`: trigger on the pull request merge
	* `on_cron`: trigger on the cron schedule, `schedule` is required, `timezone` is optional
